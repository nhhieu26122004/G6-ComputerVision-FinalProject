# üéØ K·∫æ HO·∫†CH CHI TI·∫æT: Ph√¢n lo·∫°i r√°c th·∫£i th√¥ng minh b·∫±ng YOLOv8

## üìä T√ìM T·∫ÆT V·∫§N ƒê·ªÄ

**D·ª± √°n:** H·ªá th·ªëng ph√¢n lo·∫°i r√°c th·∫£i t·ª± ƒë·ªông trong d√¢y chuy·ªÅn x·ª≠ l√Ω c√¥ng nghi·ªáp

- **Deliverable:** Code ho√†n ch·ªânh + B√°o c√°o chi ti·∫øt
- **Th·ªùi gian:** 3-4 ng√†y
- **C√¥ng ngh·ªá:** YOLOv8 (Object Detection)
- **M√¥i tr∆∞·ªùng:** Google Colab (Free GPU T4)
- **Dataset:** Ngu·ªìn c√¥ng khai tr√™n m·∫°ng
- **4 Classes:** Plastic (Nh·ª±a), Metal (Kim lo·∫°i), Paper (Gi·∫•y), Glass (Th·ªßy tinh)

---

## üó∫Ô∏è ROADMAP T·ªîNG TH·ªÇ (3-4 NG√ÄY)

```
Ng√†y 1: [Task 1 + Task 2] ‚Üí Dataset + Setup
Ng√†y 2: [Task 3] ‚Üí Training Model
Ng√†y 3: [Task 4] ‚Üí Testing + Optimization
Ng√†y 4: [Task 5] ‚Üí B√°o c√°o + Ho√†n thi·ªán
```

---

# üì¶ TASK 1: THU TH·∫¨P V√Ä CHU·∫®N B·ªä DATASET

## üîë Key Concepts

- **YOLO Format:** Annotation d·∫°ng `.txt` v·ªõi format `class_id x_center y_center width height` (normalized 0-1)
- **Data Split:** Train/Valid/Test th∆∞·ªùng theo t·ª∑ l·ªá 70/20/10 ho·∫∑c 80/10/10
- **Class Balance:** ƒê·∫£m b·∫£o 4 l·ªõp (Plastic/Metal/Paper/Glass) c√≥ s·ªë l∆∞·ª£ng ·∫£nh t∆∞∆°ng ƒë∆∞∆°ng
- **Data Augmentation:** YOLOv8 t·ª± ƒë·ªông augment khi training

## üìù C√°c b∆∞·ªõc th·ª±c hi·ªán

### B∆∞·ªõc 1.1: T√¨m ki·∫øm dataset c√¥ng khai

**Ngu·ªìn ƒë·ªÅ xu·∫•t:**

- **Roboflow Universe:** https://universe.roboflow.com (search "waste classification")
- **Kaggle:** Search "waste detection dataset" ho·∫∑c "garbage classification"
- **Specific Datasets:**
  - TrashNet Dataset
  - Waste Classification Data (Kaggle)
  - TACO Dataset (Trash Annotations in Context)

### B∆∞·ªõc 1.2: Ki·ªÉm tra v√† t·∫£i dataset

Ch·ªçn dataset c√≥:

- ‚úÖ ƒê·ªß 4 class: Plastic, Metal, Paper, Glass
- ‚úÖ Format YOLO (ho·∫∑c c√≥ th·ªÉ convert)
- ‚úÖ T·ªëi thi·ªÉu 500-1000 ·∫£nh
- ‚úÖ Annotations ch·∫•t l∆∞·ª£ng cao

### B∆∞·ªõc 1.3: Chu·∫©n b·ªã c·∫•u tr√∫c th∆∞ m·ª•c

```
dataset/
‚îú‚îÄ‚îÄ train/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îú‚îÄ‚îÄ valid/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ labels/
‚îî‚îÄ‚îÄ data.yaml
```

### B∆∞·ªõc 1.4: T·∫°o file `data.yaml`

```yaml
train: ./dataset/train/images
val: ./dataset/valid/images
test: ./dataset/test/images

nc: 4 # number of classes
names: ["Plastic", "Metal", "Paper", "Glass"]
```

### B∆∞·ªõc 1.5: Upload l√™n Google Drive

- Zip dataset v√† upload l√™n Google Drive
- ƒê·∫∑t t√™n r√µ r√†ng: `waste_detection_dataset.zip`

## üí° G·ª£i √Ω & Code h·ªØu √≠ch

**Code ki·ªÉm tra dataset:**

```python
import os
import glob

# ƒê·∫øm s·ªë ·∫£nh v√† labels
train_images = len(glob.glob('dataset/train/images/*'))
train_labels = len(glob.glob('dataset/train/labels/*'))
print(f"Train: {train_images} images, {train_labels} labels")

# Ki·ªÉm tra class distribution
class_counts = {0:0, 1:0, 2:0, 3:0}
for label_file in glob.glob('dataset/train/labels/*.txt'):
    with open(label_file, 'r') as f:
        for line in f:
            class_id = int(line.split()[0])
            class_counts[class_id] += 1

class_names = ['Plastic', 'Metal', 'Paper', 'Glass']
for i, count in class_counts.items():
    print(f"{class_names[i]}: {count} objects")
```

---

# ‚öôÔ∏è TASK 2: THI·∫æT L·∫¨P M√îI TR∆Ø·ªúNG V√Ä L√ÄM QUEN YOLOv8

## üîë Key Concepts

- **Ultralytics YOLOv8:** Library d·ªÖ s·ª≠ d·ª•ng, high-level API
- **Pre-trained Weights:** S·ª≠ d·ª•ng transfer learning t·ª´ COCO dataset
- **Model Sizes:** YOLOv8n (nano), s (small), m (medium), l (large), x (xlarge)
- **Google Colab GPU:** T4 GPU mi·ªÖn ph√≠ (12-15GB RAM)

## üìù C√°c b∆∞·ªõc th·ª±c hi·ªán

### B∆∞·ªõc 2.1: T·∫°o Google Colab Notebook

- Truy c·∫≠p https://colab.research.google.com
- T·∫°o notebook: `Waste_Classification_YOLOv8.ipynb`
- Enable GPU: `Runtime > Change runtime type > T4 GPU`

### B∆∞·ªõc 2.2: C√†i ƒë·∫∑t YOLOv8

```python
# Install Ultralytics YOLOv8
!pip install ultralytics -q

# Import libraries
from ultralytics import YOLO
import torch
import os
from google.colab import drive
from IPython.display import Image, display

# Check GPU
print("GPU Available:", torch.cuda.is_available())
print("GPU Name:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")
```

### B∆∞·ªõc 2.3: Mount Google Drive v√† load dataset

```python
# Mount Drive
drive.mount('/content/drive')

# Unzip dataset
!unzip -q /content/drive/MyDrive/waste_detection_dataset.zip -d /content/

# Verify structure
!ls -la dataset/
```

### B∆∞·ªõc 2.4: Test YOLOv8 v·ªõi pretrained model

```python
# Load pretrained YOLOv8n
model = YOLO('yolov8n.pt')

# Test inference tr√™n 1 ·∫£nh m·∫´u
results = model.predict(source='dataset/train/images/sample_001.jpg', save=True)

# Hi·ªÉn th·ªã k·∫øt qu·∫£
display(Image('runs/detect/predict/sample_001.jpg'))
```

## üí° Hi·ªÉu v·ªÅ YOLOv8 cho ng∆∞·ªùi quen CNN

**So s√°nh YOLOv8 vs CNN truy·ªÅn th·ªëng:**

- **CNN (Classification):** Input image ‚Üí Output class label (1 nh√£n)
- **YOLO (Detection):** Input image ‚Üí Output multiple [bbox, class, confidence] (nhi·ªÅu objects)

**Ki·∫øn tr√∫c YOLOv8 ƒë∆°n gi·∫£n h√≥a:**

```
Input Image (640x640)
    ‚Üì
Backbone (CSPDarknet) - gi·ªëng CNN feature extractor
    ‚Üì
Neck (FPN/PAN) - k·∫øt h·ª£p features ·ªü nhi·ªÅu scales
    ‚Üì
Head (Detection) - predict [x, y, w, h, class probabilities]
```

---

# üéì TASK 3: TRAINING MODEL YOLOV8

## üîë Key Concepts

- **Transfer Learning:** Fine-tune t·ª´ pretrained weights (COCO dataset)
- **Epochs:** S·ªë l·∫ßn model "h·ªçc" to√†n b·ªô dataset (50-100 epochs cho dataset nh·ªè)
- **Image Size (imgsz):** 640x640 l√† chu·∫©n (c√≥ th·ªÉ gi·∫£m xu·ªëng 416 n·∫øu GPU y·∫øu)
- **Batch Size:** S·ªë ·∫£nh x·ª≠ l√Ω c√πng l√∫c (16-32 cho Colab T4)
- **Learning Rate:** T·ªëc ƒë·ªô h·ªçc (YOLOv8 t·ª± ƒë·ªông tune)

## üìù C√°c b∆∞·ªõc th·ª±c hi·ªán

### B∆∞·ªõc 3.1: Ch·ªçn model size ph√π h·ª£p

**Khuy·∫øn ngh·ªã cho 3-4 ng√†y:**

- `yolov8n.pt` (Nano): Fastest, 3.2M params - **ƒê·ªÄ XU·∫§T cho newbie**
- `yolov8s.pt` (Small): Balance, 11.2M params - N·∫øu mu·ªën accuracy cao h∆°n

### B∆∞·ªõc 3.2: C·∫•u h√¨nh training parameters

```python
from ultralytics import YOLO

# Load pretrained model
model = YOLO('yolov8n.pt')  # ho·∫∑c yolov8s.pt

# Training
results = model.train(
    data='dataset/data.yaml',      # Path to data.yaml
    epochs=100,                      # S·ªë epochs
    imgsz=640,                       # Image size
    batch=16,                        # Batch size (gi·∫£m xu·ªëng 8 n·∫øu OOM)
    patience=20,                     # Early stopping patience
    save=True,                       # Save checkpoints
    device=0,                        # GPU device (0 = first GPU)
    project='runs/waste_detection',  # Save location
    name='yolov8n_waste',           # Experiment name
    exist_ok=True,                   # Overwrite existing
    pretrained=True,                 # Use pretrained weights
    optimizer='AdamW',               # Optimizer
    verbose=True,                    # Print training info
    seed=42,                         # Reproducibility

    # Augmentation (c√≥ th·ªÉ b·∫≠t n·∫øu dataset nh·ªè)
    hsv_h=0.015,                     # Hue augmentation
    hsv_s=0.7,                       # Saturation
    hsv_v=0.4,                       # Value
    degrees=10.0,                    # Rotation
    translate=0.1,                   # Translation
    scale=0.5,                       # Scaling
    flipud=0.0,                      # Flip up-down
    fliplr=0.5,                      # Flip left-right
    mosaic=1.0,                      # Mosaic augmentation
)
```

### B∆∞·ªõc 3.3: Monitor training

```python
# Training s·∫Ω t·ª± ƒë·ªông hi·ªÉn th·ªã:
# - Loss curves (box_loss, cls_loss, dfl_loss)
# - Metrics (Precision, Recall, mAP50, mAP50-95)
# - Validation results

# Sau khi training xong, xem results:
!ls runs/waste_detection/yolov8n_waste/

# File quan tr·ªçng:
# - weights/best.pt (model t·ªët nh·∫•t)
# - weights/last.pt (model cu·ªëi c√πng)
# - results.csv (metrics theo epochs)
# - confusion_matrix.png
```

### B∆∞·ªõc 3.4: Visualize training results

```python
from IPython.display import Image

# Loss curves
display(Image('runs/waste_detection/yolov8n_waste/results.png'))

# Confusion matrix
display(Image('runs/waste_detection/yolov8n_waste/confusion_matrix.png'))

# Sample predictions
display(Image('runs/waste_detection/yolov8n_waste/val_batch0_pred.jpg'))
```

## üí° Tips ƒë·ªÉ training hi·ªáu qu·∫£

**N·∫øu training b·ªã Out of Memory (OOM):**

```python
# Gi·∫£m batch size
batch=8  # ho·∫∑c 4

# Ho·∫∑c gi·∫£m image size
imgsz=416  # thay v√¨ 640
```

**Theo d√µi training t·ª´ xa (tr√°nh Colab disconnect):**

```python
# Sao l∆∞u weights ƒë·ªãnh k·ª≥ v√†o Drive
import shutil

# Sau m·ªói 10 epochs, copy weights sang Drive
!cp runs/waste_detection/yolov8n_waste/weights/best.pt \
    /content/drive/MyDrive/yolov8_waste_backup.pt
```

---

# üß™ TASK 4: TESTING, EVALUATION V√Ä OPTIMIZATION

## üîë Key Concepts

- **mAP (mean Average Precision):** Metric ch√≠nh cho object detection
  - mAP@0.5: IoU threshold = 0.5 (d·ªÖ h∆°n)
  - mAP@0.5:0.95: IoU t·ª´ 0.5-0.95 (kh√≥ h∆°n, chu·∫©n COCO)
- **Precision:** T·ª∑ l·ªá predictions ƒë√∫ng trong s·ªë predictions (ƒë·ªô ch√≠nh x√°c)
- **Recall:** T·ª∑ l·ªá objects ƒë∆∞·ª£c detect trong t·ªïng s·ªë objects (ƒë·ªô ph·ªß)
- **Confusion Matrix:** Ma tr·∫≠n nh·∫ßm l·∫´n gi·ªØa c√°c classes

## üìù C√°c b∆∞·ªõc th·ª±c hi·ªán

### B∆∞·ªõc 4.1: Load model ƒë√£ train

```python
# Load best model
model = YOLO('runs/waste_detection/yolov8n_waste/weights/best.pt')
```

### B∆∞·ªõc 4.2: Evaluate tr√™n test set

```python
# Validation
metrics = model.val(
    data='dataset/data.yaml',
    split='test',           # Evaluate on test set
    imgsz=640,
    batch=16,
    save_json=True,         # Save COCO format results
    save_hybrid=True,       # Save labels + predictions
    conf=0.25,              # Confidence threshold
    iou=0.6,                # IoU threshold for NMS
    plots=True              # Generate plots
)

# Print metrics
print("\n=== EVALUATION RESULTS ===")
print(f"mAP@0.5: {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
print(f"Precision: {metrics.box.mp:.4f}")
print(f"Recall: {metrics.box.mr:.4f}")

# Per-class metrics
print("\n=== PER-CLASS METRICS ===")
class_names = ['Plastic', 'Metal', 'Paper', 'Glass']
for i, name in enumerate(class_names):
    print(f"{name}: mAP@0.5 = {metrics.box.maps[i]:.4f}")
```

### B∆∞·ªõc 4.3: Test tr√™n ·∫£nh th·ª±c t·∫ø

```python
# Predict tr√™n test images
results = model.predict(
    source='dataset/test/images',
    conf=0.25,              # Confidence threshold
    iou=0.6,                # IoU for NMS
    save=True,              # Save results
    save_txt=True,          # Save labels
    save_conf=True,         # Save confidence scores
    show_labels=True,       # Show labels
    show_conf=True,         # Show confidence
    line_width=2,           # Bounding box thickness
    project='runs/test',
    name='final_results'
)

# Hi·ªÉn th·ªã m·ªôt s·ªë k·∫øt qu·∫£
import glob
test_results = glob.glob('runs/test/final_results/*.jpg')[:5]
for img_path in test_results:
    display(Image(img_path))
```

### B∆∞·ªõc 4.4: T·∫°o demo video/stream (Optional)

```python
# Predict tr√™n video
results = model.predict(
    source='demo_video.mp4',
    save=True,
    stream=True,            # Stream results
    conf=0.3
)

# Ho·∫∑c webcam (n·∫øu c√≥)
# results = model.predict(source=0, show=True)
```

### B∆∞·ªõc 4.5: Optimization - Tinh ch·ªânh threshold

```python
# Th·ª≠ nghi·ªám v·ªõi confidence threshold kh√°c nhau
confidence_thresholds = [0.1, 0.25, 0.5, 0.7]

for conf in confidence_thresholds:
    print(f"\n=== Testing with confidence = {conf} ===")
    metrics = model.val(
        data='dataset/data.yaml',
        conf=conf,
        plots=False
    )
    print(f"mAP@0.5: {metrics.box.map50:.4f}, Precision: {metrics.box.mp:.4f}, Recall: {metrics.box.mr:.4f}")
```

## üí° Ph√¢n t√≠ch k·∫øt qu·∫£

**Ti√™u ch√≠ ƒë√°nh gi√° t·ªët cho b√†i t·∫≠p l·ªõn:**

- ‚úÖ **mAP@0.5 ‚â• 0.70** (70%+): Kh√° t·ªët
- ‚úÖ **mAP@0.5:0.95 ‚â• 0.50** (50%+): Ch·∫•p nh·∫≠n ƒë∆∞·ª£c
- ‚úÖ **Precision ‚â• 0.75**: √çt false positives
- ‚úÖ **Recall ‚â• 0.70**: Ph√°t hi·ªán ƒë∆∞·ª£c ƒëa s·ªë objects

**N·∫øu k·∫øt qu·∫£ ch∆∞a t·ªët:**

1. TƒÉng epochs (100 ‚Üí 150-200)
2. D√πng model l·ªõn h∆°n (yolov8n ‚Üí yolov8s)
3. TƒÉng data augmentation
4. Ki·ªÉm tra l·∫°i ch·∫•t l∆∞·ª£ng annotations

---

# üìù TASK 5: VI·∫æT B√ÅO C√ÅO V√Ä HO√ÄN THI·ªÜN

## üîë Key Concepts

- **Reproducibility:** B√°o c√°o ph·∫£i ƒë·ªß chi ti·∫øt ƒë·ªÉ ng∆∞·ªùi kh√°c t√°i hi·ªán ƒë∆∞·ª£c
- **Visualization:** D√πng bi·ªÉu ƒë·ªì, h√¨nh ·∫£nh minh h·ªça
- **Scientific Writing:** Ng√¥n ng·ªØ chuy√™n nghi·ªáp, r√µ r√†ng
- **Code Documentation:** Comment code, README r√µ r√†ng

## üìù C√°c b∆∞·ªõc th·ª±c hi·ªán

### B∆∞·ªõc 5.1: C·∫•u tr√∫c b√°o c√°o

**M·ª•c l·ª•c b√°o c√°o (8-15 trang):**

```markdown
1. GI·ªöI THI·ªÜU
   1.1. B·ªëi c·∫£nh v√† ƒë·ªông l·ª±c
   1.2. M·ª•c ti√™u nghi√™n c·ª©u
   1.3. Ph·∫°m vi ƒë·ªÅ t√†i

2. C∆† S·ªû L√ù THUY·∫æT
   2.1. Object Detection
   2.2. YOLO (You Only Look Once)
   2.3. YOLOv8 Architecture
   2.4. Transfer Learning

3. PH∆Ø∆†NG PH√ÅP TH·ª∞C HI·ªÜN
   3.1. Dataset

   - Ngu·ªìn d·ªØ li·ªáu
   - Th·ªëng k√™ dataset
   - Data preprocessing
     3.2. M√¥i tr∆∞·ªùng th·ª±c nghi·ªám
     3.3. C·∫•u h√¨nh training
     3.4. Evaluation metrics

4. K·∫æT QU·∫¢ V√Ä TH·∫¢O LU·∫¨N
   4.1. K·∫øt qu·∫£ training

   - Loss curves
   - Metrics theo epochs
     4.2. K·∫øt qu·∫£ testing
   - mAP, Precision, Recall
   - Per-class performance
   - Confusion matrix
     4.3. Visualizations
     4.4. Ph√¢n t√≠ch v√† nh·∫≠n x√©t

5. K·∫æT LU·∫¨N V√Ä H∆Ø·ªöNG PH√ÅT TRI·ªÇN
   5.1. T√≥m t·∫Øt k·∫øt qu·∫£
   5.2. H·∫°n ch·∫ø
   5.3. H∆∞·ªõng ph√°t tri·ªÉn

6. T√ÄI LI·ªÜU THAM KH·∫¢O

7. PH·ª§ L·ª§C
   - Source code ch√≠nh
   - Th√¥ng s·ªë chi ti·∫øt
```

### B∆∞·ªõc 5.2: Thu th·∫≠p t√†i li·ªáu cho b√°o c√°o

**Artifacts c·∫ßn c√≥:**

```python
# T·∫°o folder b√°o c√°o
!mkdir -p report_artifacts

# Copy c√°c file quan tr·ªçng
!cp runs/waste_detection/yolov8n_waste/results.png report_artifacts/
!cp runs/waste_detection/yolov8n_waste/confusion_matrix.png report_artifacts/
!cp runs/waste_detection/yolov8n_waste/val_batch0_pred.jpg report_artifacts/
!cp runs/test/final_results/*.jpg report_artifacts/sample_predictions/

# Export metrics to CSV
import pandas as pd
metrics_df = pd.read_csv('runs/waste_detection/yolov8n_waste/results.csv')
metrics_df.to_excel('report_artifacts/training_metrics.xlsx', index=False)

# Zip to√†n b·ªô
!zip -r report_artifacts.zip report_artifacts/
```

### B∆∞·ªõc 5.3: Vi·∫øt c√°c ph·∫ßn ch√≠nh

**Section 1: Gi·ªõi thi·ªáu (1-2 trang)**

```markdown
## 1. GI·ªöI THI·ªÜU

### 1.1. B·ªëi c·∫£nh v√† ƒë·ªông l·ª±c

Trong b·ªëi c·∫£nh √¥ nhi·ªÖm m√¥i tr∆∞·ªùng ng√†y c√†ng nghi√™m tr·ªçng, vi·ªác ph√¢n lo·∫°i
v√† x·ª≠ l√Ω r√°c th·∫£i hi·ªáu qu·∫£ tr·ªü th√†nh v·∫•n ƒë·ªÅ c·∫•p thi·∫øt. H·ªá th·ªëng ph√¢n lo·∫°i
r√°c t·ª± ƒë·ªông s·ª≠ d·ª•ng th·ªã gi√°c m√°y t√≠nh c√≥ th·ªÉ gi√∫p t·ªëi ∆∞u h√≥a quy tr√¨nh
t√°i ch·∫ø trong c√°c nh√† m√°y x·ª≠ l√Ω r√°c...

### 1.2. M·ª•c ti√™u nghi√™n c·ª©u

- X√¢y d·ª±ng m√¥ h√¨nh deep learning ph√°t hi·ªán v√† ph√¢n lo·∫°i 4 lo·∫°i r√°c
- ƒê·∫°t ƒë·ªô ch√≠nh x√°c mAP@0.5 ‚â• 70%
- ·ª®ng d·ª•ng ƒë∆∞·ª£c trong m√¥i tr∆∞·ªùng th·ª±c t·∫ø (d√¢y chuy·ªÅn c√¥ng nghi·ªáp)
```

**Section 2: C∆° s·ªü l√Ω thuy·∫øt (2-3 trang)**

```markdown
## 2. C∆† S·ªû L√ù THUY·∫æT

### 2.2. YOLO (You Only Look Once)

YOLO l√† h·ªç m√¥ h√¨nh object detection s·ª≠ d·ª•ng approach "single-stage",
d·ª± ƒëo√°n bounding boxes v√† class probabilities tr·ª±c ti·∫øp t·ª´ full image
trong m·ªôt l·∫ßn forward pass. ƒêi·ªÅu n√†y gi√∫p YOLO nhanh h∆°n ƒë√°ng k·ªÉ so v·ªõi
two-stage detectors nh∆∞ R-CNN...

[Ch√®n h√¨nh: YOLO architecture diagram]

### 2.3. YOLOv8 Architecture

YOLOv8 (2023) l√† phi√™n b·∫£n m·ªõi nh·∫•t v·ªõi nh·ªØng c·∫£i ti·∫øn:

- Backbone: CSPDarknet v·ªõi C2f modules
- Neck: PAN (Path Aggregation Network)
- Head: Decoupled head (classification & localization ri√™ng bi·ªát)
- Loss: VFL (Varifocal Loss) + CIoU loss

[Ch√®n c√¥ng th·ª©c to√°n h·ªçc n·∫øu c·∫ßn]
```

**Section 4: K·∫øt qu·∫£ (3-4 trang)**

```markdown
## 4. K·∫æT QU·∫¢ V√Ä TH·∫¢O LU·∫¨N

### 4.1. K·∫øt qu·∫£ Training

Model ƒë∆∞·ª£c training trong 100 epochs v·ªõi early stopping patience = 20.
H√¨nh 4.1 th·ªÉ hi·ªán s·ª± h·ªôi t·ª• c·ªßa c√°c loss functions v√† metrics.

[Ch√®n h√¨nh: results.png - loss curves v√† metrics]

Nh·∫≠n x√©t:

- Loss gi·∫£m ƒë·ªÅu, kh√¥ng c√≥ d·∫•u hi·ªáu overfitting
- mAP@0.5 ƒë·∫°t 0.78 t·∫°i epoch 87 (best)
- Precision v√† Recall c√¢n b·∫±ng (~0.75)

### 4.2. K·∫øt qu·∫£ Testing

B·∫£ng 4.1: Performance metrics tr√™n test set

| Metric       | Value |
| ------------ | ----- |
| mAP@0.5      | 0.782 |
| mAP@0.5:0.95 | 0.534 |
| Precision    | 0.756 |
| Recall       | 0.724 |
| F1-Score     | 0.740 |

[Ch√®n h√¨nh: Confusion Matrix]

B·∫£ng 4.2: Per-class Performance

| Class   | mAP@0.5 | Precision | Recall |
| ------- | ------- | --------- | ------ |
| Plastic | 0.812   | 0.789     | 0.756  |
| Metal   | 0.798   | 0.812     | 0.742  |
| Paper   | 0.745   | 0.701     | 0.698  |
| Glass   | 0.773   | 0.723     | 0.701  |

Nh·∫≠n x√©t:

- Plastic v√† Metal ƒë∆∞·ª£c ph√°t hi·ªán t·ªët nh·∫•t (ƒë·∫∑c tr∆∞ng r√µ r√†ng)
- Paper c√≥ performance th·∫•p nh·∫•t (d·ªÖ nh·∫ßm v·ªõi background)
- Model ho·∫°t ƒë·ªông ·ªïn ƒë·ªãnh tr√™n c·∫£ 4 classes

### 4.3. Visualizations

[Ch√®n 4-6 h√¨nh ·∫£nh predictions t·ªët v√† c√≥ issues]

H√¨nh 4.3: M·ªôt s·ªë tr∆∞·ªùng h·ª£p predictions th√†nh c√¥ng
H√¨nh 4.4: C√°c tr∆∞·ªùng h·ª£p false positives v√† false negatives
```

### B∆∞·ªõc 5.4: T·ªï ch·ª©c code v√† t√†i li·ªáu

**C·∫•u tr√∫c folder submission:**

```
submission/
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ train.py                    # Training script
‚îÇ   ‚îú‚îÄ‚îÄ test.py                     # Testing script
‚îÇ   ‚îú‚îÄ‚îÄ inference.py                # Inference demo
‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt            # Dependencies
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îî‚îÄ‚îÄ Waste_Classification_YOLOv8.ipynb
‚îú‚îÄ‚îÄ report/
‚îÇ   ‚îú‚îÄ‚îÄ report.pdf                  # B√°o c√°o ch√≠nh
‚îÇ   ‚îî‚îÄ‚îÄ presentation.pptx           # Slides thuy·∫øt tr√¨nh (n·∫øu c·∫ßn)
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îú‚îÄ‚îÄ weights/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ best.pt                 # Trained model
‚îÇ   ‚îú‚îÄ‚îÄ plots/                      # Visualizations
‚îÇ   ‚îî‚îÄ‚îÄ metrics/                    # CSV, logs
‚îú‚îÄ‚îÄ dataset/
‚îÇ   ‚îî‚îÄ‚îÄ data.yaml                   # Dataset config (kh√¥ng upload ·∫£nh n·∫øu l·ªõn)
‚îî‚îÄ‚îÄ README.md                       # H∆∞·ªõng d·∫´n ch·∫°y
```

**Template README.md:**

````markdown
# Waste Classification System using YOLOv8

## üìå Overview

H·ªá th·ªëng ph√¢n lo·∫°i r√°c th·∫£i t·ª± ƒë·ªông s·ª≠ d·ª•ng YOLOv8 cho d√¢y chuy·ªÅn x·ª≠ l√Ω c√¥ng nghi·ªáp.

## üéØ Performance

- **mAP@0.5:** 0.782
- **Precision:** 0.756
- **Recall:** 0.724
- **Classes:** Plastic, Metal, Paper, Glass

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```
````

### 2. Download Dataset

[Link to dataset ho·∫∑c h∆∞·ªõng d·∫´n]

### 3. Training

```bash
python code/train.py --data dataset/data.yaml --epochs 100
```

### 4. Testing

```bash
python code/test.py --weights results/weights/best.pt
```

### 5. Inference

```bash
python code/inference.py --source path/to/image.jpg
```

## üìä Results

[Ch√®n h√¨nh k·∫øt qu·∫£ ch√≠nh]

## üë• Team Members

- [T√™n th√†nh vi√™n 1]
- [T√™n th√†nh vi√™n 2]
- ...

## üìö References

1. Ultralytics YOLOv8: https://github.com/ultralytics/ultralytics
2. [Dataset source]
3. [Papers...]

````

### B∆∞·ªõc 5.5: Code scripts ch√≠nh

**File: `code/train.py`**
```python
"""
Training script for Waste Classification using YOLOv8
"""
from ultralytics import YOLO
import argparse

def train(args):
    # Load model
    model = YOLO(args.model)

    # Train
    results = model.train(
        data=args.data,
        epochs=args.epochs,
        imgsz=args.imgsz,
        batch=args.batch,
        device=args.device,
        project=args.project,
        name=args.name,
        patience=20,
        save=True,
        pretrained=True,
        optimizer='AdamW',
        verbose=True,
        seed=42
    )

    print("Training completed!")
    print(f"Best model saved at: {args.project}/{args.name}/weights/best.pt")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--model', type=str, default='yolov8n.pt', help='Model size')
    parser.add_argument('--data', type=str, required=True, help='Path to data.yaml')
    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs')
    parser.add_argument('--imgsz', type=int, default=640, help='Image size')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--device', type=str, default='0', help='Device (0 or cpu)')
    parser.add_argument('--project', type=str, default='runs/train', help='Project path')
    parser.add_argument('--name', type=str, default='waste_yolov8', help='Experiment name')

    args = parser.parse_args()
    train(args)
````

**File: `code/test.py`**

```python
"""
Testing script for Waste Classification
"""
from ultralytics import YOLO
import argparse
import json

def test(args):
    # Load model
    model = YOLO(args.weights)

    # Validate
    metrics = model.val(
        data=args.data,
        split='test',
        imgsz=args.imgsz,
        batch=args.batch,
        conf=args.conf,
        iou=args.iou,
        save_json=True,
        plots=True
    )

    # Print results
    print("\n" + "="*50)
    print("EVALUATION RESULTS")
    print("="*50)
    print(f"mAP@0.5:      {metrics.box.map50:.4f}")
    print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
    print(f"Precision:    {metrics.box.mp:.4f}")
    print(f"Recall:       {metrics.box.mr:.4f}")
    print("="*50)

    # Save results
    results_dict = {
        'map50': float(metrics.box.map50),
        'map': float(metrics.box.map),
        'precision': float(metrics.box.mp),
        'recall': float(metrics.box.mr)
    }

    with open('test_results.json', 'w') as f:
        json.dump(results_dict, f, indent=4)

    print("\nResults saved to test_results.json")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, required=True, help='Path to trained weights')
    parser.add_argument('--data', type=str, required=True, help='Path to data.yaml')
    parser.add_argument('--imgsz', type=int, default=640, help='Image size')
    parser.add_argument('--batch', type=int, default=16, help='Batch size')
    parser.add_argument('--conf', type=float, default=0.25, help='Confidence threshold')
    parser.add_argument('--iou', type=float, default=0.6, help='IoU threshold')

    args = parser.parse_args()
    test(args)
```

**File: `code/inference.py`**

```python
"""
Inference script for single image/video
"""
from ultralytics import YOLO
import argparse
import cv2

def inference(args):
    # Load model
    model = YOLO(args.weights)

    # Predict
    results = model.predict(
        source=args.source,
        conf=args.conf,
        iou=args.iou,
        save=args.save,
        show=args.show,
        line_width=2,
        show_labels=True,
        show_conf=True
    )

    # Print detections
    class_names = ['Plastic', 'Metal', 'Paper', 'Glass']

    for i, r in enumerate(results):
        print(f"\n--- Image {i+1} ---")
        boxes = r.boxes
        for box in boxes:
            cls = int(box.cls[0])
            conf = float(box.conf[0])
            print(f"{class_names[cls]}: {conf:.2f}")

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--weights', type=str, required=True, help='Path to weights')
    parser.add_argument('--source', type=str, required=True, help='Image/video/directory path')
    parser.add_argument('--conf', type=float, default=0.25, help='Confidence threshold')
    parser.add_argument('--iou', type=float, default=0.6, help='IoU threshold')
    parser.add_argument('--save', action='store_true', help='Save results')
    parser.add_argument('--show', action='store_true', help='Show results')

    args = parser.parse_args()
    inference(args)
```

**File: `requirements.txt`**

```
ultralytics>=8.0.0
torch>=2.0.0
torchvision>=0.15.0
opencv-python>=4.8.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
Pillow>=10.0.0
tqdm>=4.65.0
```

## üí° Tips vi·∫øt b√°o c√°o chuy√™n nghi·ªáp

### Ng√¥n ng·ªØ v√† tr√¨nh b√†y:

- ‚úÖ S·ª≠ d·ª•ng thu·∫≠t ng·ªØ ti·∫øng Anh cho technical terms, gi·∫£i th√≠ch ti·∫øng Vi·ªát
- ‚úÖ Tr√≠ch d·∫´n papers v√† sources ƒë·∫ßy ƒë·ªß (IEEE/ACM format)
- ‚úÖ ƒê√°nh s·ªë h√¨nh, b·∫£ng, c√¥ng th·ª©c r√µ r√†ng
- ‚úÖ Font ch·ªØ: Times New Roman 13pt, Calibri 12pt ho·∫∑c Arial 11pt
- ‚úÖ Line spacing: 1.5

### Visualization tips:

```python
# T·∫°o publication-quality figures
import matplotlib.pyplot as plt
import seaborn as sns

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.dpi'] = 300  # High resolution
plt.rcParams['font.size'] = 12

# Example: Plot training metrics
import pandas as pd

metrics_df = pd.read_csv('runs/waste_detection/yolov8n_waste/results.csv')

fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# Loss curves
axes[0, 0].plot(metrics_df['train/box_loss'], label='Train Box Loss')
axes[0, 0].set_xlabel('Epoch')
axes[0, 0].set_ylabel('Loss')
axes[0, 0].set_title('Box Loss Over Time')
axes[0, 0].legend()
axes[0, 0].grid(True)

# mAP curves
axes[0, 1].plot(metrics_df['metrics/mAP50(B)'], label='mAP@0.5', color='green')
axes[0, 1].set_xlabel('Epoch')
axes[0, 1].set_ylabel('mAP')
axes[0, 1].set_title('mAP Over Time')
axes[0, 1].legend()
axes[0, 1].grid(True)

# Precision & Recall
axes[1, 0].plot(metrics_df['metrics/precision(B)'], label='Precision', color='blue')
axes[1, 0].plot(metrics_df['metrics/recall(B)'], label='Recall', color='orange')
axes[1, 0].set_xlabel('Epoch')
axes[1, 0].set_ylabel('Score')
axes[1, 0].set_title('Precision & Recall')
axes[1, 0].legend()
axes[1, 0].grid(True)

# F1 Score
f1_scores = 2 * (metrics_df['metrics/precision(B)'] * metrics_df['metrics/recall(B)']) / \
            (metrics_df['metrics/precision(B)'] + metrics_df['metrics/recall(B)'])
axes[1, 1].plot(f1_scores, label='F1-Score', color='red')
axes[1, 1].set_xlabel('Epoch')
axes[1, 1].set_ylabel('F1-Score')
axes[1, 1].set_title('F1-Score Over Time')
axes[1, 1].legend()
axes[1, 1].grid(True)

plt.tight_layout()
plt.savefig('report_artifacts/training_summary.png', dpi=300, bbox_inches='tight')
plt.show()
```

---

# ‚úÖ TI√äU CH√ç ƒê√ÅNH GI√Å CU·ªêI C√ôNG (Acceptable Output Criteria)

## üìä Ti√™u ch√≠ k·ªπ thu·∫≠t

### 1. Model Performance (40%)

- ‚úÖ **mAP@0.5 ‚â• 0.70** (Excellent: ‚â•0.80)
- ‚úÖ **mAP@0.5:0.95 ‚â• 0.45** (Excellent: ‚â•0.55)
- ‚úÖ **Precision ‚â• 0.70**
- ‚úÖ **Recall ‚â• 0.65**
- ‚úÖ C√°c class ph·∫£i balanced (kh√¥ng c√≥ class n√†o qu√° k√©m)

### 2. Code Quality (20%)

- ‚úÖ Code ch·∫°y ƒë∆∞·ª£c, kh√¥ng l·ªói
- ‚úÖ C√≥ comments v√† docstrings r√µ r√†ng
- ‚úÖ C·∫•u tr√∫c folder logic, d·ªÖ hi·ªÉu
- ‚úÖ README h∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß
- ‚úÖ requirements.txt ƒë·∫ßy ƒë·ªß dependencies

### 3. B√°o c√°o (30%)

- ‚úÖ ƒê·∫ßy ƒë·ªß c√°c ph·∫ßn: Gi·ªõi thi·ªáu, L√Ω thuy·∫øt, Ph∆∞∆°ng ph√°p, K·∫øt qu·∫£, K·∫øt lu·∫≠n
- ‚úÖ C√≥ visualizations (loss curves, confusion matrix, predictions)
- ‚úÖ Ph√¢n t√≠ch k·∫øt qu·∫£ s√¢u s·∫Øc, kh√¥ng ch·ªâ li·ªát k√™ s·ªë li·ªáu
- ‚úÖ Ng√¥n ng·ªØ chuy√™n nghi·ªáp, kh√¥ng l·ªói ch√≠nh t·∫£
- ‚úÖ Tr√≠ch d·∫´n ƒë·∫ßy ƒë·ªß (‚â•5 t√†i li·ªáu tham kh·∫£o)
- ‚úÖ Format chu·∫©n h·ªçc thu·∫≠t (10-15 trang)

### 4. Presentation & Demo (10%)

- ‚úÖ Slides tr√¨nh b√†y r√µ r√†ng (n·∫øu c·∫ßn thuy·∫øt tr√¨nh)
- ‚úÖ Demo inference tr√™n ·∫£nh th·ª±c t·∫ø
- ‚úÖ Video demo ng·∫Øn (1-2 ph√∫t) - optional nh∆∞ng impressive

## üéØ Checklist tr∆∞·ªõc khi n·ªôp

### Code Checklist:

- [ ] Code train.py ch·∫°y ƒë∆∞·ª£c tr√™n Colab
- [ ] Code test.py cho ra k·∫øt qu·∫£ ƒë√∫ng
- [ ] Code inference.py demo ƒë∆∞·ª£c tr√™n ·∫£nh m·ªõi
- [ ] requirements.txt ƒë·∫ßy ƒë·ªß
- [ ] README.md h∆∞·ªõng d·∫´n r√µ r√†ng
- [ ] Model weights (best.pt) ƒë∆∞·ª£c l∆∞u

### B√°o c√°o Checklist:

- [ ] Trang b√¨a c√≥ ƒë·∫ßy ƒë·ªß th√¥ng tin (t√™n ƒë·ªÅ t√†i, nh√≥m, m√¥n h·ªçc)
- [ ] M·ª•c l·ª•c c√≥ s·ªë trang
- [ ] T·∫•t c·∫£ h√¨nh/b·∫£ng c√≥ caption v√† ƒë√°nh s·ªë
- [ ] T·∫•t c·∫£ h√¨nh/b·∫£ng ƒë∆∞·ª£c refer trong text
- [ ] Ph·∫ßn References format chu·∫©n
- [ ] Kh√¥ng c√≥ l·ªói ch√≠nh t·∫£
- [ ] File PDF d∆∞·ªõi 20MB

### Results Checklist:

- [ ] Training converged (loss gi·∫£m, kh√¥ng diverge)
- [ ] mAP@0.5 ‚â• 0.70
- [ ] Confusion matrix reasonable (kh√¥ng bias qu√°)
- [ ] Sample predictions ch·∫•t l∆∞·ª£ng t·ªët
- [ ] Per-class metrics acceptable

---

# üöÄ TIPS TH√ÄNH C√îNG

## Qu·∫£n l√Ω th·ªùi gian 3-4 ng√†y:

**Ng√†y 1 (8 gi·ªù):**

- S√°ng: T√¨m dataset, setup Colab, test YOLOv8 c∆° b·∫£n (3h)
- Chi·ªÅu: Chu·∫©n b·ªã dataset, t·∫°o data.yaml, start training (3h)
- T·ªëi: Nghi√™n c·ª©u l√Ω thuy·∫øt cho b√°o c√°o (2h)

**Ng√†y 2 (8 gi·ªù):**

- S√°ng: Monitor training, ƒëi·ªÅu ch·ªânh n·∫øu c·∫ßn (2h)
- Chi·ªÅu: Training xong, analyze results (3h)
- T·ªëi: Vi·∫øt ph·∫ßn Ph∆∞∆°ng ph√°p c·ªßa b√°o c√°o (3h)

**Ng√†y 3 (8 gi·ªù):**

- S√°ng: Testing, optimization, t·∫°o visualizations (4h)
- Chi·ªÅu: Vi·∫øt ph·∫ßn K·∫øt qu·∫£ v√† K·∫øt lu·∫≠n (3h)
- T·ªëi: Ho√†n thi·ªán code scripts (1h)

**Ng√†y 4 (6 gi·ªù):**

- S√°ng: Vi·∫øt ph·∫ßn Gi·ªõi thi·ªáu v√† L√Ω thuy·∫øt (2h)
- Chi·ªÅu: Review to√†n b·ªô, fix l·ªói, format b√°o c√°o (3h)
- T·ªëi: Final check, export PDF, chu·∫©n b·ªã submission (1h)

## Tr√°nh nh·ªØng sai l·∫ßm th∆∞·ªùng g·∫∑p:

‚ùå **KH√îNG N√äN:**

- Training v·ªõi qu√° √≠t epochs (< 50)
- B·ªè qua vi·ªác validate model
- B√°o c√°o ch·ªâ copy-paste l√Ω thuy·∫øt kh√¥ng ph√¢n t√≠ch
- Kh√¥ng backup weights (Colab disconnect l√† m·∫•t h·∫øt)
- D√πng dataset qu√° nh·ªè (< 300 ·∫£nh)

‚úÖ **N√äN:**

- Backup weights v√†o Drive ƒë·ªãnh k·ª≥
- Test nhi·ªÅu confidence thresholds
- Ph√¢n t√≠ch c·ª• th·ªÉ t·ª´ng class performance
- ƒê∆∞a v√≠ d·ª• c·ª• th·ªÉ (·∫£nh predictions)
- Th·ª´a nh·∫≠n limitations v√† ƒë·ªÅ xu·∫•t improvements

## Resources h·ªØu √≠ch:

1. **YOLOv8 Documentation:** https://docs.ultralytics.com
2. **Roboflow Blog:** https://blog.roboflow.com (nhi·ªÅu tutorials)
3. **Papers to cite:**
   - Original YOLO: Redmon et al., "You Only Look Once" (2016)
   - YOLOv8: Ultralytics YOLOv8 (2023)
4. **Dataset sources:**
   - Roboflow Universe: https://universe.roboflow.com
   - Kaggle Datasets: https://www.kaggle.com/datasets

---

# üìû TROUBLESHOOTING

## V·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p:

### 1. Colab Out of Memory (OOM)

**Gi·∫£i ph√°p:**

```python
# Gi·∫£m batch size
batch=8  # ho·∫∑c 4

# Gi·∫£m image size
imgsz=416

# Restart runtime v√† clear cache
import torch
torch.cuda.empty_cache()
```

### 2. Colab Disconnect gi·ªØa ch·ª´ng

**Gi·∫£i ph√°p:**

```python
# Th√™m v√†o ƒë·∫ßu notebook:
from google.colab import drive
drive.mount('/content/drive')

# Trong training code:
# Th√™m callback ƒë·ªÉ save m·ªói 10 epochs
callbacks = {
    'on_train_epoch_end': lambda: shutil.copy(
        'runs/.../weights/last.pt',
        '/content/drive/MyDrive/backup.pt'
    )
}
```

### 3. mAP qu√° th·∫•p (< 0.50)

**Nguy√™n nh√¢n & gi·∫£i ph√°p:**

- Dataset qu√° nh·ªè ‚Üí T√¨m dataset l·ªõn h∆°n ho·∫∑c augment nhi·ªÅu
- Annotations k√©m ‚Üí Ki·ªÉm tra l·∫°i labels
- Training ch∆∞a ƒë·ªß ‚Üí TƒÉng epochs
- Model qu√° nh·ªè ‚Üí D√πng yolov8s thay v√¨ yolov8n

### 4. Training kh√¥ng converge (loss kh√¥ng gi·∫£m)

**Ki·ªÉm tra:**

```python
# Verify data ƒë∆∞·ª£c load ƒë√∫ng
from ultralytics import YOLO
model = YOLO('yolov8n.pt')

# Test tr√™n 1 batch
results = model.train(data='data.yaml', epochs=1, batch=1, imgsz=640)

# N·∫øu ch·∫°y ƒë∆∞·ª£c ‚Üí tƒÉng d·∫ßn batch v√† epochs
```

---

# üéì K·∫æT LU·∫¨N

V·ªõi k·∫ø ho·∫°ch chi ti·∫øt n√†y, b·∫°n c√≥ th·ªÉ ho√†n th√†nh b√†i t·∫≠p l·ªõn trong 3-4 ng√†y v·ªõi ch·∫•t l∆∞·ª£ng t·ªët.

**Key success factors:**

1. ‚è∞ **Time management:** Tu√¢n th·ªß timeline ch·∫∑t ch·∫Ω
2. üîç **Dataset quality:** Ch·ªçn dataset t·ªët ngay t·ª´ ƒë·∫ßu
3. üíæ **Backup:** Lu√¥n backup weights v√† code
4. üìä **Analysis:** Ph√¢n t√≠ch k·∫øt qu·∫£ s√¢u s·∫Øc, kh√¥ng ch·ªâ li·ªát k√™ s·ªë
5. üìù **Documentation:** Code v√† b√°o c√°o r√µ r√†ng, chuy√™n nghi·ªáp

**Expected final results:**

- Trained YOLOv8 model v·ªõi mAP@0.5 ~ 0.75-0.85
- B√°o c√°o 10-15 trang ch·∫•t l∆∞·ª£ng cao
- Code base s·∫°ch s·∫Ω, d·ªÖ reproduce
- Demo impresssive v·ªõi predictions ch√≠nh x√°c

Ch√∫c b·∫°n th·ª±c hi·ªán project th√†nh c√¥ng! üöÄ

---

**L∆∞u √Ω cu·ªëi:** Document n√†y l√† roadmap, trong qu√° tr√¨nh th·ª±c hi·ªán c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh linh ho·∫°t d·ª±a tr√™n k·∫øt qu·∫£ th·ª±c t·∫ø. ƒê·ª´ng ng·∫ßn ng·∫°i experiment v√† t√¨m hi·ªÉu th√™m!
